------------------------------------------------------------
Saving location: results/walker2d-expert-v2|exp_1|diffusion-ql|T-5|lr_decay|ms-offline|k-1|0
------------------------------------------------------------
/home/tori/miniconda3/envs/DQL/lib/python3.9/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
24-12-17.15:49|Variant:
24-12-17.15:49|{
  "exp": "exp_1",
  "device": "cuda:0",
  "env_name": "walker2d-expert-v2",
  "dir": "results",
  "seed": 0,
  "num_steps_per_epoch": 1000,
  "batch_size": 256,
  "lr_decay": true,
  "early_stop": true,
  "save_best_model": false,
  "discount": 0.99,
  "tau": 0.005,
  "T": 5,
  "beta_schedule": "vp",
  "algo": "ql",
  "ms": "offline",
  "dual_diffusion": false,
  "output_dir": "results",
  "num_epochs": 100,
  "eval_freq": 50,
  "eval_episodes": 10,
  "lr": 0.0003,
  "eta": 1.0,
  "max_q_backup": false,
  "reward_tune": "no",
  "gn": 5.0,
  "top_k": 1,
  "version": "Diffusion-Policies-RL",
  "state_dim": 17,
  "action_dim": 6,
  "max_action": 1.0
}
------------------------------------------------------------
Env: walker2d-expert-v2, state_dim: 17, action_dim: 6, dual_diffusion: False
------------------------------------------------------------
load datafile: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [00:01<00:00, 16.28it/s]
------------------------------------------------------------
Loaded buffer
------------------------------------------------------------
******************************************************************************************
Training Start
******************************************************************************************
Traceback (most recent call last):
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/main.py", line 282, in <module>
    train_agent(env,
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/main.py", line 90, in train_agent
    loss_metric = agent.train(data_sampler,
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/agents/ql_diffusion.py", line 158, in train
    new_action = self.actor(state)
  File "/home/tori/miniconda3/envs/DQL/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/tori/miniconda3/envs/DQL/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/agents/diffusion.py", line 182, in forward
    return self.sample(state, *args, **kwargs)
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/agents/diffusion.py", line 144, in sample
    action = self.p_sample_loop(state, shape, *args, **kwargs)
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/agents/diffusion.py", line 127, in p_sample_loop
    x = self.p_sample(x, timesteps, state)
  File "/home/tori/Documents/research/Diffusion-Policies-for-Offline-RL/agents/diffusion.py", line 113, in p_sample
    return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise
KeyboardInterrupt
